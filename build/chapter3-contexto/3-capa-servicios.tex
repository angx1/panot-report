\section{Servicios Externos}

\label{sec:capa-servicios}

Además de proporcionar la infraestructura de base de datos y autenticación, \textit{Supabase} se ha utilizado como plataforma 
de backend completo para gestionar todas las conexiones con servicios externos mediante su arquitectura de \textit{Edge Functions} \cite{supabase-edge-functions}, 
que actúa como un \gls{api-gateway} centralizado. Esta decisión arquitectónica se alinea directamente con los principios de seguridad 
y eficiencia por diseño establecidos en el proyecto.

Las \textit{Edge Functions} de \textit{Supabase} son funciones serverless escritas en TypeScript y ejecutadas en un runtime basado en \textit{Deno}. 
Estas funciones son distribuidas y replicadas globalmente en servidores para reducir así la latencia en función del posicionamiento geográfico del usuario y la demanda de recursos. 
El flujo de ejecución de una función es el siguiente:

{\small
\begin{enumerate}
    \item \textbf{Entrada de solicitud en el edge gateway}: El \gls{api-gateway} actúa como \gls{relay} que enruta el tráfico, gestiona los 
    headers de autenticación, valida los \acrfull{jwt} y aplica reglas de enrutamiento y control de tráfico. De esta manera, se centralizan las comprobaciones de 
    seguridad antes de ejecutar el código, lo que garantiza que solo solicitudes autenticadas y autorizadas accedan a los servicios.
    
    \item \textbf{Ejecución en el edge runtime}: La función se ejecuta en el nodo \textit{Edge Runtime} más cercano al usuario, minimizando la latencia de procesamiento.
    
    \item \textbf{Respuesta a través del gateway}: El \gls{api-gateway} reenvía la respuesta al cliente y registra los metadatos de la 
    solicitud para análisis en logs y métricas que pueden explorarse en el Dashboard de \textit{Supabase} o integrarse con sistemas de monitorización como se verá más adelante.
\end{enumerate}
}

\begin{figure}[H]
    \centering
    \includegraphics[width=.7\textwidth]{figures/api-gateway-flow.png}
    \caption{Arquitectura \gls{api-gateway} simplificada}
    \label{fig:flujo-api-gateway}
\end{figure}

Esta arquitectura de \gls{api-gateway} \ref{fig:flujo-api-gateway} de las \textit{Edge Functions} de \textit{Supabase} proporciona múltiples beneficios que justifican su adopción:

{\small
\begin{itemize}
    \item \textbf{Seguridad centralizada}: Todas las claves de \acrshort{api} y credenciales de servicios externos se mantienen en el 
    servidor, nunca expuestas al cliente móvil. El gateway valida automáticamente la autenticación antes de procesar cualquier 
    solicitud, reduciendo significativamente la superficie de ataque.
    
    \item \textbf{Baja latencia}: La distribución global de las funciones garantiza que las solicitudes se procesen en el nodo 
    más cercano geográficamente al usuario, reduciendo la latencia total de la solicitud.
    
    \item \textbf{Escalabilidad automática}: Las Edge Functions se escalan automáticamente según la demanda, eliminando la 
    necesidad de gestionar infraestructura de servidores y optimizando los costes operativos mediante un modelo de pago por uso.
    
    \item \textbf{Simplificación del desarrollo}: La integración nativa con las \acrshort{api}s de \textit{Supabase} permite que las funciones 
    accedan directamente a la base de datos, autenticación y almacenamiento sin configuración adicional, reduciendo la complejidad 
    del código y la gestión de credenciales.
\end{itemize}
}

Alternativas considiredas como las \textit{AWS Lambda} de Amazon, \textit{Google Cloud Functions} de Google y \textit{Azure Functions} de Microsoft se han descartado por su complejidad y coste ya que 
no se contaba con experiencia previa con estas plataformas y se ha optado por la simplicidad y facilidad de integración que ofrece \textit{Supabase}. Además, elegir las \textit{Edge Functions} de \textit{Supabase} es una decisión  
resultaba la opción más orgnánica y natural para el proyecto teniendo en cuenta el contexto tecnológico de la capa de datos \ref{sec:capa-datos}.

\subsection{Subcapa de Inteligencia Artificial}

La subcapa de inteligencia artificial de PANOT, responsable del procesamiento de interacciones y la generación de relaciones 
contextuales, se ha implementado mediante \textit{Supabase Edge Functions} \ref{sec:capa-servicios} que actúan como intermediario entre la aplicación móvil 
y la \textit{API de OpenAI}, utilizando el modelo {\footnotesize \texttt{gpt-5-mini}} para el procesamiento de lenguaje natural.

La elección de \textit{OpenAI} como proveedor de servicios de inteligencia artificial se fundamenta en su alta adopción por parte 
de la comunidad de desarrolladores y su extensa documentación, que facilita, de nuevo, la integración y resolución de problemas durante el 
desarrollo. \textit{OpenAI} mantiene una hoja de ruta activa con actualizaciones frecuentes de sus modelos y mejoras continuas en la \textit{API de OpenAI}, garantizando 
la evolución y mantenibilidad a largo plazo de la integración.

La selección del modelo específico {\footnotesize \texttt{gpt-5-mini}} se fundamenta en su equilibrio entre eficiencia, calidad y coste. Este modelo ofrece tiempos de respuesta y costes 
menores que alternativas más grandes como {\footnotesize \texttt{gpt-5-pro}} o {\footnotesize \texttt{gpt-5.1}}, permitiendo una 
experiencia de usuario más fluida en aplicaciones móviles donde la latencia es crítica. A pesar de ser más ligero, mantiene 
capacidades de comprensión y generación de lenguaje natural suficientes para las tareas requeridas en PANOT. Además, su coste por token sustancialmente menor 
es fundamental para la sostenibilidad económica del proyecto, considerando que las operaciones de inteligencia artificial se 
ejecutan frecuentemente en segundo plano.

\vspace{1cm}
\begin{figure}[H]
    \centering
    \includegraphics[width=.95\textwidth]{figures/comparativa-modelos-openai.png}
    \caption{Comparativa de modelos mencionados de OpenAI. Tabla sacada de la documentación de OpenAI \cite{openai-models-compare}.}
    \label{fig:comparativa-modelos-openai}
\end{figure}

Las \textit{Edge Functions} encapsulan la lógica de comunicación con la \textit{API de OpenAI}, incluyendo manejo de errores, gestión de 
timeouts y validación de la estructura de respuesta, abstrayéndose así de acoplar la lógica de comunicación con el cliente móvil además de una capa de observabilidad de las peticiones 
como se verá más adelante. El sistema de agentes que orquesta este procesamiento se ha desarrollado utilizando \textit{LangChain} como framework y su diseño y funcionamiento se detallan en la Sección~\ref{sec:sistema-multiagente}.

Como alternativas se consideraron los modelos \textit{Claude} de \textit{Anthropic}, que ofrece capacidades avanzadas de razonamiento pero con un coste 
de computación sustancialmente mayor. No obstante, se decidió elegir \textit{OpenAI}, teniendo en cuenta la experiencia previa que se tenía con la plataforma.

\subsection{Subcapa de Infraestructura de Pagos}

Para la integración de funcionalidades de pago y suscripciones, se ha seleccionado \textit{Stripe} como proveedor de los servicios 
de pasarela de pagos. \textit{Stripe} es una plataforma \acrfull{baas} que proporciona \acrshort{api}s robustas y seguras para procesar pagos, 
gestionar suscripciones recurrentes y manejar la facturación.

La integración con \textit{Stripe} se realiza también mediante \textit{Supabase Edge Functions} que gestionan la creación 
de intenciones de pago y el procesamiento de transacciones de forma segura, además de integrar \gls{webhook}s para manejar eventos asíncronos como confirmaciones de pago o actualizaciones de métodos de pago, sincronizando 
automáticamente el estado en la base de datos. Adicionalmente, \textit{Stripe} gestiona automáticamente el cumplimiento de estándares de seguridad 
como \gls{pci-dss} (Payment Card Industry Data Security Standard) lo que garantiza una mayor seguridad en las transacciones del usuario.

Como alternativa a \textit{Stripe} se evaluó \textit{Polar}, otra plataforma \acrshort{baas} que ofrece funcionalidades similares para el 
procesamiento de transacciones. La principal diferencia entre ambas plataformas radica en que \textit{Polar}, aunque ofrece una mejor experiencia de 
desarrollo y una simplicidad de implementación mayor a \textit{Stripe}, está desarrollada fundamentalmente para aplicaciones web, mientras que \textit{Stripe} 
proporciona un soporte nativo y para aplicaciones móviles con \acrfull{sdk}s específicos para \textit{Expo}. \textit{Polar} 
no cuenta con suficiente adopción en el ecosistema de aplicaciones móviles, lo que se traduce en documentación limitada, menos ejemplos de integración y 
una comunidad de desarrolladores más reducida para este tipo de aplicaciones. Esto incrementaría el tiempo de desarrollo y el riesgo de problemas de compatibilidad. 

\subsection{Subcapa de Observabilidad}

Una parte crítica en el desarrollo de productos digitales es disponer de un entendimiento claro de cómo los usuarios interactúan con el producto y de cómo se comportan los sistemas que lo sustentan. 
En PANOT, la observabilidad se aborda en dos dimensiones: el análisis de uso y comportamiento del usuario en la aplicación, y el seguimiento del sistema de agentes que orquesta el 
procesamiento de inteligencia relacional (Sección~\ref{sec:sistema-multiagente}). Para cada una se ha integrado una herramienta específica: \textit{PostHog} para la analítica de producto y \textit{LangSmith} 
para la observabilidad del sistema agéntico.

Para el análisis de uso de funcionalidades y comprensión del comportamiento de los usuarios es esencial que los desarrolladores tomen decisiones 
informadas basadas en datos reales y conocer qué aspectos de la aplicación aportan valor al usuario y cuales no. Para ello se ha integrado \textit{PostHog}, una plataforma de análisis de productos de código abierto que 
proporciona la capacidad de hacer un seguimiento de eventos una vez la aplicación está en manos del usuario.

La elección de \textit{PostHog} se fundamenta en su compatibilidad con React Native y Expo, su modelo de código abierto, y su enfoque específico en análisis de producto que proporciona métricas relevantes 
para la toma de decisiones basadas en datos. Además, \textit{PostHog} ofrece una capa gratuita que permite comenzar el seguimiento de eventos sin coste inicial, lo que se ajusta al contexto de este proyecto. Este seguimiento de eventos se implementa mediante llamadas a 
funciones específicas que permiten registrar eventos personalizados con propiedades asociadas. 

Junto con la analítica de producto, se ha incorporado \textit{LangSmith} como capa de observabilidad del sistema de agentes desarrollado con \textit{LangChain}. \textit{LangSmith} es la plataforma de 
trazabilidad y depuración del ecosistema LangChain y permite monitorizar en tiempo real cada ejecución del sistema de agentes: latencia de las llamadas, tokens consumidos, 
coste por ejecución y flujo completo de invocaciones entre cada agente. Esta visibilidad es fundamental para validar requisitos de rendimiento y eficiencia económica, 
depurar fallos y optimizar el uso del modelo de lenguaje.

Para la analítica de producto se consideraron \textit{Mixpanel}, que ofrece herramientas avanzadas para el análisis de productos y retención de usuarios con un \acrshort{sdk} robusto para 
React Native, y \textit{Amplitude}, una plataforma líder en análisis de productos que proporciona capacidades avanzadas de \gls{analisis-cohorte} y \gls{embudos}. Sin embargo, ambas 
presentan modelos más restrictivos en sus planes gratuitos comparado con \textit{PostHog}, y requieren una configuración más compleja para su integración con Expo. 

En cuanto a la observabilidad del sistema de agentes, no se ha considerado ninguna alternativa ya que \textit{LangSmith} es la plataforma de trazabilidad por defecto del 
ecosistema LangChain y resultaba la opción más coherente.
